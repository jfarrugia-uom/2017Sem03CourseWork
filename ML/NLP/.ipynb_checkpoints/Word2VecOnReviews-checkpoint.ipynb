{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec example with gensim, NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb as mysql\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import re # regular expression library\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download text datasets including stop words\n",
    "#nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hook up to mysql\n",
    "# to fix ascii problem when tokenising, important to specify character set\n",
    "# https://stackoverflow.com/questions/21129020/how-to-fix-unicodedecodeerror-ascii-codec-cant-decode-byte\n",
    "db = mysql.connect(\"localhost\", \"jfarrugia\", \"jfarrugia\", \"yelp_db\", charset='utf8',\n",
    "use_unicode=True)\n",
    "# load some data from a previously created table\n",
    "pd_review = pd.read_sql(\"select id, name, text, stars from toronto_50K_random_reviews\", con=db)\n",
    "\n",
    "# close connection\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm review shape\n",
    "pd_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show 1 review\n",
    "pd_review[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# details from https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words\n",
    "# lower case all text\n",
    "lc_review = pd_review[\"text\"][0].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split one review into separate words\n",
    "words = lc_review.split(\" \")\n",
    "# remove stop words from review text\n",
    "words_no_stop = [w for w in words if w not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# removel morphological affices from words, leaving word stem\n",
    "stemmer = PorterStemmer()\n",
    "words_no_stop_stem = [stemmer.stem(w) for w in words_no_stop]\n",
    "print words_no_stop_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_review(base_review, remove_stop=False, stem = False, join=False):\n",
    "    words = re.sub(\"[^a-zA-Z0-9]\", \" \", base_review) \n",
    "    # convert to lower case + split    \n",
    "    words = words.lower().split(\" \")    \n",
    "    # searching a set is faster than a list    \n",
    "    # might contemplate tweaking stop word list\n",
    "    #stop = {x for x in set(stopwords.words(\"english\")) if x not in ['not', 'no']\n",
    "    if remove_stop:\n",
    "        stop = set(stopwords.words(\"english\"))\n",
    "        words = [word for word in words if word not in stop]\n",
    "    # run porter stemmer\n",
    "    if stem:\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "    # return string\n",
    "    if join:\n",
    "        return \" \".join(words)\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test one review\n",
    "print process_review(pd_review[\"text\"][0])\n",
    "clean_reviews = pd_review[\"text\"].apply(process_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now create bag-of-words with vectoriser\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# limit vocab to 5000 words for now\n",
    "cv = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None,\n",
    "                     stop_words = None, max_features=5000)\n",
    "\n",
    "review_features = cv.fit_transform(clean_reviews)\n",
    "# convert from sparse matrix to numpy array\n",
    "review_features = review_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check size of bag of words model\n",
    "print review_features.shape\n",
    "# have a look at the vocab\n",
    "#print cv.get_feature_names()\n",
    "\n",
    "def get_top_n_features(bow, cv, n):\n",
    "    weights = bow.mean(axis=0).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'term': cv.get_feature_names(), 'weight':weights})\n",
    "    print weights_df.sort_values(by='weight', ascending=False).head(n)    \n",
    "\n",
    "# print 50 top terms\n",
    "get_top_n_features(review_features, cv, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec requires review paragraphs split into individual sentences\n",
    "# the datastructure to hold this data is a list of lists - \n",
    "# inner list holds sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK's punkt includes a pre-trained tokenizer for english which can\n",
    "# be used to transform (split) new paragraph observations into sentences\n",
    "punkt = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split review corpus into sentences\n",
    "# cannot use clean_reviews since punctuation was removed\n",
    "\n",
    "#process_review(pd_review[\"text\"][0], False, False, False)\n",
    "def split_to_sentence(base_reviews, tokeniser, remove_stop=False):\n",
    "    raw_sentences = tokeniser.tokenize(base_reviews.strip())\n",
    "    sentences = []\n",
    "    for rs in raw_sentences:\n",
    "        # consider only strings with length >= 1\n",
    "        if (len(rs) > 0):\n",
    "            sentences.append( process_review(rs, remove_stop=remove_stop) )\n",
    "    return sentences\n",
    "\n",
    "sentences = pd_review[\"text\"].apply(lambda x: split_to_sentence(x, punkt)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'my', u'pasta', u'primavera', u'was', u'nice', u'', u'and', u'the', u'soup', u'special', u'we', u'had', u'was', u'delicious', u'']\n"
     ]
    }
   ],
   "source": [
    "# we need to flatten sentences list since we have a triple level list\n",
    "# that we need to convert to a list of lists (2 levels)\n",
    "sentence_list = [item for sublist in sentences for item in sublist]\n",
    "\n",
    "# format will be ok with word2vector\n",
    "print sentence_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444454\n"
     ]
    }
   ],
   "source": [
    "# we have aroiund 444000 sentences minded from 50K reviews of\n",
    "# Toronto restaurants\n",
    "print len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it to have clean messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 30   # Minimum word count                        \n",
    "num_workers = 2       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# train word2vec model based on my 50K review sample\n",
    "\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentence_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# in case we need to port model without re-training\n",
    "model_name = \"200features_30minwords_10context\"\n",
    "model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should we need to load the model\n",
    "model = word2vec.Word2Vec.load(\"200features_30minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'chinese', 0.7390034794807434),\n",
       " (u'malaysian', 0.7228488922119141),\n",
       " (u'szechuan', 0.720445990562439),\n",
       " (u'hakka', 0.716964066028595),\n",
       " (u'shanghai', 0.7040140628814697),\n",
       " (u'mein', 0.6960002183914185),\n",
       " (u'kong', 0.6846320629119873),\n",
       " (u'hong', 0.6804869174957275),\n",
       " (u'northern', 0.6803617477416992),\n",
       " (u'korean', 0.6744514107704163)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# terms most similar to sushi\n",
    "#print model.wv.most_similar(\"sushi\")\n",
    "# terms most similar to sushi\n",
    "model.wv.most_similar(\"cantonese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6793, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.wv.syn0 consists of a feature vector for each work\n",
    "type(model.wv.syn0)\n",
    "# with a min word count of 30, a vocab of 6,793 words as created\n",
    "len(model.wv.vocab)\n",
    "# shape of wv.syn0 should be 6793, 200\n",
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'alla', 0.8697019815444946),\n",
       " (u'pappardelle', 0.8591920733451843),\n",
       " (u'carbonara', 0.8361091613769531),\n",
       " (u'gnocchi', 0.8305134773254395),\n",
       " (u'fettuccine', 0.8223904967308044),\n",
       " (u'rigatoni', 0.820175290107727),\n",
       " (u'penne', 0.8139102458953857),\n",
       " (u'spaghetti', 0.8118501901626587),\n",
       " (u'ravioli', 0.8116596937179565),\n",
       " (u'tagliatelle', 0.8072959184646606)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 200 dimension feature vector returned for word 'italian'\n",
    "model.wv[\"italian\"].shape\n",
    "model.wv.most_similar(positive=['pasta','chinese'], negative=['italian'])\n",
    "#model.wv.most_similar('disgusting')\n",
    "#model.wv.most_similar('fresh')\n",
    "#model.wv.most_similar(positive=['kobe','beef'])\n",
    "\n",
    "model.wv.most_similar(\"bolognese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a feature vector composed of the average of word vectors in\n",
    "# a review's paragraph\n",
    "def convert_review_feature_vector(word_list, model, feature_count):\n",
    "    # initialise array of length feature_count (200 )\n",
    "    feature_vector = np.zeros((feature_count,), dtype='float32')\n",
    "    # stores count of words that are features in learned vocab\n",
    "    word_count = 0.\n",
    "    # convert learned vocab to set for faster processing\n",
    "    vocab_set = set(model.wv.index2word)\n",
    "    # iterate over words in word_list, adding feature vectors together\n",
    "    for word in word_list:\n",
    "        if word in vocab_set:\n",
    "            word_count += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    # finally divide feature_vector by number of words ot get arithmetic vector mean\n",
    "    feature_vector = np.divide(feature_vector, word_count)\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_reviews2 = pd_review[\"text\"].apply(lambda x: process_review(x, remove_stop=True))\n",
    "# creates a 2D array of feature vector of size review count x feature count\n",
    "review_vectors =\\\n",
    "np.array(clean_reviews2.apply(lambda x: \n",
    "                              convert_review_feature_vector(x, model, 200)).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for search:  3.03672599792 seconds.\n"
     ]
    }
   ],
   "source": [
    "# execute this code to compare each individual review with the search string\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "search_string = \"cantonese\"\n",
    "\n",
    "search_vect = convert_review_feature_vector(search_string.split(), model, 200)\n",
    "\n",
    "from scipy import spatial\n",
    "# calculate cosine similarity of search string with review vectors\n",
    "distances = []\n",
    "for rv in review_vectors:\n",
    "    distances.append(np.round(spatial.distance.cosine(search_vect, rv),3))\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for search: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3055  5150 33156 ... 14855 22600 24955]\n",
      "(u'Chopstick House', u'nPatYo3wQ7tcvx7nzOU4GQ', 0.331)\n",
      "(u'Ajisen Ramen', u'6SAfQKe2oM5g_EtcYXyAMg', 0.366)\n",
      "(u\"Lee's Thai Spring Roll\", u'uaCYXxCsZSD3KMg8XiOdwg', 0.393)\n",
      "(u'Chinese Dumpling House', u'ag8gM2YKZkjndCvl2ti7kQ', 0.41)\n",
      "(u'Kaiju', u'6EVBc9kdc3Hd8KZkLVPnGA', 0.428)\n",
      "(u'Lotus Garden Hakka Indian Style Chinese', u'TBzgzTFSa7pJXiLD7emYaQ', 0.429)\n",
      "(u'Green Papaya', u'kM91Woq__EKVzLjo4dOTaw', 0.433)\n",
      "(u'Bi Bim Bap', u'ruR-mrEaNbFJGnM-WCbcgg', 0.433)\n",
      "(u'Pho Vietnam', u'1Epby_tsFDci4sP0Nbjwsw', 0.445)\n",
      "(u'Seor Ak San', u'4twpbw7n4DmsLxAm6-sMkg', 0.447)\n",
      "(u'Lime Asian Cuisine', u'Lft-0Xy72YbwRkn_n5hfXA', 0.453)\n",
      "(u'Jim Chai Kee Wonton Noodle', u'X9ftU-exKhTMOjtr3B52rw', 0.462)\n",
      "(u'Sansotei', u'-BbnAc9YEO6pjvJGEtFbVQ', 0.463)\n",
      "(u'Rol San', u'O1TvPrgkK2bUo5O5aSZ7lw', 0.465)\n",
      "(u'Ho Su Bistro', u'QTSCFDPcuROE8UCvGS8Fiw', 0.465)\n",
      "(u'Sabai Sabai Kitchen and Bar', u'DPA9MQKkCqT0qnevsG740A', 0.465)\n",
      "(u'Flip Toss & Thai Kitchen', u'Et9tn7nEpEs043pQVa2HZg', 0.47)\n",
      "(u'New Sky Restaurant', u'J_btDyZbIv0hZNjrw56zlA', 0.47)\n",
      "(u'Banh Mi Boys', u'N93EYZy9R0sdlEvubu94ig', 0.47)\n",
      "(u'Yummy Korean Food Restaurant', u'F_oPMHJrH42R67xp5eKtQA', 0.473)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>nPatYo3wQ7tcvx7nzOU4GQ</td>\n",
       "      <td>Chopstick House</td>\n",
       "      <td>I've tried two other hakka Chinese places, but...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>nPatYo3wQ7tcvx7nzOU4GQ</td>\n",
       "      <td>Chopstick House</td>\n",
       "      <td>The food was awful, I love Hakka food but the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39338</th>\n",
       "      <td>nPatYo3wQ7tcvx7nzOU4GQ</td>\n",
       "      <td>Chopstick House</td>\n",
       "      <td>I think I had the worst Hakka Food ever. So ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id             name  \\\n",
       "901    nPatYo3wQ7tcvx7nzOU4GQ  Chopstick House   \n",
       "3055   nPatYo3wQ7tcvx7nzOU4GQ  Chopstick House   \n",
       "39338  nPatYo3wQ7tcvx7nzOU4GQ  Chopstick House   \n",
       "\n",
       "                                                    text  stars  \n",
       "901    I've tried two other hakka Chinese places, but...      5  \n",
       "3055   The food was awful, I love Hakka food but the ...      1  \n",
       "39338  I think I had the worst Hakka Food ever. So ba...      1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.argsort(distances)\n",
    "# print top 20 cosine similarity\n",
    "results = [(pd_review[\"name\"][x], pd_review[\"id\"][x], distances[x]) for x in np.argsort(distances)[:20]]\n",
    "for result in results:\n",
    "    print result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a newer technique which first concatenates all reviews for a particular\n",
    "# resto together.  \n",
    "# the review dataframe row count is reduced to the number of restaurants.\n",
    "# the aggregated review becomes our new document\n",
    "\n",
    "# first group by resto id and aggregate reviews by first converting to list\n",
    "# and then joining\n",
    "concat_query = pd_review.groupby('id')['text'].apply(lambda x: \" \".join(list(x)))\n",
    "# extract unique id restaurant tuples from original dataframe\n",
    "uniq_restaurants = pd_review.loc[:,[\"id\",\"name\"]].drop_duplicates()\n",
    "# join aggregated reviews with unique resto data frame\n",
    "joint_reviews = uniq_restaurants.join(concat_query, on=\"id\").reset_index(drop=True)\n",
    "\n",
    "clean_reviews3 = joint_reviews[\"text\"].apply(lambda x: process_review(x, remove_stop=True))\n",
    "# creates a 2D array of feature vector of size review count x feature count\n",
    "review_vectors2 =\\\n",
    "np.array(clean_reviews3.apply(lambda x: \n",
    "                              convert_review_feature_vector(x, model, 200)).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for search:  0.368130922318 seconds.\n"
     ]
    }
   ],
   "source": [
    "# execute this code to compare each individual review with the search string\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "search_string = \"crumpet cream\"\n",
    "\n",
    "search_vect = convert_review_feature_vector(search_string.split(), model, 200)\n",
    "\n",
    "from scipy import spatial\n",
    "# calculate cosine similarity of search string with review vectors\n",
    "distances = []\n",
    "for rv in review_vectors2:\n",
    "    distances.append(np.round(spatial.distance.cosine(search_vect, rv),3))\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for search: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Oven Fresh', u'hjZ4cVn3PZk_1hqfoxMXXg', 0.561)\n",
      "(u\"Summer's Ice Cream\", u'fPISsMIXOYZP1uins2Bwyw', 0.574)\n",
      "(u'Real Fruit Bubble Tea', u'ZPI1t-WcZruILq7OQYnZHg', 0.584)\n",
      "(u\"Zelden's Deli and Desserts\", u'_eRcc1OFDbi3fnVBskXP9g', 0.599)\n",
      "(u\"Menchie's Frozen Yogurt\", u'lnZhnZzBiG5rD9brVv5uXA', 0.622)\n",
      "(u'Fugo Desserts', u'EK38MXW_OsC5CZVvIZodIw', 0.64)\n",
      "(u\"Annie's Tea House\", u'kBYBXh7M9wKXvHgluJWiLQ', 0.642)\n",
      "(u'Sugar N Spice Cafe', u'2nW_hHaOt0DpsL1lPpuMXA', 0.646)\n",
      "(u'The Cups', u'z9SjyM0Ixr1ud8zI7Y93-A', 0.647)\n",
      "(u'Fresh Cup Bubble Tea', u'aeEVWgcUf1-f46Dk-EzgTQ', 0.654)\n",
      "(u'Sugar Marmalade', u'2PCz_uVX7GOXtGHNXAPXhw', 0.654)\n",
      "(u'WaffleBar', u'XlUgRG9zJsqBKpPztvlpYQ', 0.654)\n",
      "(u'The Mad Italian', u'Ik3CEeFyMFDnqrhc30NubQ', 0.655)\n",
      "(u'FUEL+', u'GJX_96Dldb0JZLUlHBgAcA', 0.659)\n",
      "(u'Petit Nuage', u'XC23vvqHdCJqev-bmeh2HQ', 0.664)\n",
      "(u\"Shopsy's Sports Grill\", u'C3Q_yp1ylwXOvj8M-1-OLw', 0.664)\n",
      "(u'Oscar Coffee & Espresso Bar', u'1nEYOQZAbtGa42ZGZ5fnaQ', 0.679)\n",
      "(u'Nova Era Bakery', u'd_NZqSypBaSs-2-r32CWrw', 0.68)\n",
      "(u'Revitasize', u'56z-cq7L0MPE-nU9D-UEyQ', 0.684)\n",
      "(u'Starbucks', u'1-Z6tVozu4v1JHzlg8x4kA', 0.685)\n"
     ]
    }
   ],
   "source": [
    "# print top 20 cosine similarity\n",
    "results = [(joint_reviews[\"name\"][x], joint_reviews[\"id\"][x], distances[x]) for x in np.argsort(distances)[:20]]\n",
    "for result in results:\n",
    "    print result\n",
    "    \n",
    "# mixed results here.  The more reviews there are for a few place, the more\n",
    "# penalised the restaurant is.  The mean of the review's representation in vector\n",
    "# space depends on the total number of words. \n",
    "# sometimes shorter reviews (or less reviews) come up trumps\n",
    "# on the other hand, we avoid duplice results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle original reviews, review_vectors for use in application\n",
    "import pickle;\n",
    "pickle_out = open (\"pd_review.pkl\", \"wb\")\n",
    "pickle.dump(pd_review, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open (\"review_vector.pkl\", \"wb\")\n",
    "pickle.dump(review_vectors, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to project onto 2D using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_closestwords(model, word, feature_count):\n",
    "    \n",
    "    arr = np.empty((0,feature_count), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    #np.set_printoptions(suppress=True)\n",
    "    result = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = result[:, 0]\n",
    "    y_coords = result[:, 1]\n",
    "    # display scatter plot\n",
    "    #fig = plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
