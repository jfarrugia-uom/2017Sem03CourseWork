{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec example with gensim, NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb as mysql\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import re # regular expression library\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download text datasets including stop words\n",
    "#nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hook up to mysql\n",
    "# to fix ascii problem when tokenising, important to specify character set\n",
    "# https://stackoverflow.com/questions/21129020/how-to-fix-unicodedecodeerror-ascii-codec-cant-decode-byte\n",
    "ip = \"localhost\"\n",
    "username = 'jfarrugia'\n",
    "password = 'jfarrugia'\n",
    "\n",
    "db = mysql.connect(ip, username, password, \"yelp_db\", charset='utf8',\n",
    "use_unicode=True)\n",
    "# load some data from a previously created table\n",
    "pd_review = pd.read_sql(\"select id, name, text, stars from toronto_50K_random_reviews\", con=db)\n",
    "\n",
    "# close connection\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm review shape\n",
    "pd_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Not too bad overall. Got the 4 course menu which included a whole fish (grilled or seared) an appetizer and desert. The fish was wonderful. The pita appetizer was ok but one of the dips was not very good. I can't comment on dessert as I didn't try it. The others at my table seemed to think it was ok. The service was pretty good. Nothing special. We did think it was odd that the waiter spoke to people at the front door while he was taking our order. Not too bad overall. Would go back but won't be sprinting.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 1 review\n",
    "pd_review[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# details from https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words\n",
    "# lower case all text\n",
    "lc_review = pd_review[\"text\"][0].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split one review into separate words\n",
    "words = lc_review.split(\" \")\n",
    "# remove stop words from review text\n",
    "words_no_stop = [w for w in words if w not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bad', u'overall.', u'got', u'4', u'cours', u'menu', u'includ', u'whole', u'fish', u'(grill', u'seared)', u'appet', u'desert.', u'fish', u'wonderful.', u'pita', u'appet', u'ok', u'one', u'dip', u'good.', u\"can't\", u'comment', u'dessert', u'tri', u'it.', u'other', u'tabl', u'seem', u'think', u'ok.', u'servic', u'pretti', u'good.', u'noth', u'special.', u'think', u'odd', u'waiter', u'spoke', u'peopl', u'front', u'door', u'take', u'order.', u'bad', u'overall.', u'would', u'go', u'back', u'sprinting.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# removel morphological affices from words, leaving word stem\n",
    "stemmer = PorterStemmer()\n",
    "words_no_stop_stem = [stemmer.stem(w) for w in words_no_stop]\n",
    "print words_no_stop_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_review(base_review, remove_stop=False, stem = False, join=False):\n",
    "    words = re.sub(\"[^a-zA-Z0-9]\", \" \", base_review) \n",
    "    # convert to lower case + split    \n",
    "    words = words.lower().split(\" \")    \n",
    "    # searching a set is faster than a list    \n",
    "    # might contemplate tweaking stop word list\n",
    "    #stop = {x for x in set(stopwords.words(\"english\")) if x not in ['not', 'no']\n",
    "    if remove_stop:\n",
    "        stop = set(stopwords.words(\"english\"))\n",
    "        words = [word for word in words if word not in stop]\n",
    "    # run porter stemmer\n",
    "    if stem:\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "    # return string\n",
    "    if join:\n",
    "        return \" \".join(words)\n",
    "    else:\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec requires review paragraphs split into individual sentences\n",
    "# the datastructure to hold this data is a list of lists - \n",
    "# inner list holds sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK's punkt includes a pre-trained tokenizer for english which can\n",
    "# be used to transform (split) new paragraph observations into sentences\n",
    "punkt = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split review corpus into sentences\n",
    "# cannot use clean_reviews since punctuation was removed\n",
    "\n",
    "#process_review(pd_review[\"text\"][0], False, False, False)\n",
    "def split_to_sentence(base_reviews, tokeniser, remove_stop=False):\n",
    "    raw_sentences = tokeniser.tokenize(base_reviews.strip())\n",
    "    sentences = []\n",
    "    for rs in raw_sentences:\n",
    "        # consider only strings with length >= 1\n",
    "        if (len(rs) > 0):\n",
    "            sentences.append( process_review(rs, remove_stop=remove_stop) )\n",
    "    return sentences\n",
    "\n",
    "sentences = pd_review[\"text\"].apply(lambda x: split_to_sentence(x, punkt)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'my', u'pasta', u'primavera', u'was', u'nice', u'', u'and', u'the', u'soup', u'special', u'we', u'had', u'was', u'delicious', u'']\n"
     ]
    }
   ],
   "source": [
    "# we need to flatten sentences list since we have a triple level list\n",
    "# that we need to convert to a list of lists (2 levels)\n",
    "sentence_list = [item for sublist in sentences for item in sublist]\n",
    "\n",
    "# format will be ok with word2vector\n",
    "print sentence_list[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444454\n"
     ]
    }
   ],
   "source": [
    "# we have aroiund 444000 sentences minded from 50K reviews of\n",
    "# Toronto restaurants\n",
    "print len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-27 16:53:10,030 : INFO : collecting all words and their counts\n",
      "2018-01-27 16:53:10,033 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-27 16:53:10,086 : INFO : PROGRESS: at sentence #10000, processed 166483 words, keeping 9036 word types\n",
      "2018-01-27 16:53:10,140 : INFO : PROGRESS: at sentence #20000, processed 332568 words, keeping 12587 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-27 16:53:10,203 : INFO : PROGRESS: at sentence #30000, processed 497948 words, keeping 15344 word types\n",
      "2018-01-27 16:53:10,263 : INFO : PROGRESS: at sentence #40000, processed 664084 words, keeping 17603 word types\n",
      "2018-01-27 16:53:10,314 : INFO : PROGRESS: at sentence #50000, processed 829665 words, keeping 19661 word types\n",
      "2018-01-27 16:53:10,356 : INFO : PROGRESS: at sentence #60000, processed 997906 words, keeping 21318 word types\n",
      "2018-01-27 16:53:10,405 : INFO : PROGRESS: at sentence #70000, processed 1168609 words, keeping 23082 word types\n",
      "2018-01-27 16:53:10,455 : INFO : PROGRESS: at sentence #80000, processed 1333955 words, keeping 24453 word types\n",
      "2018-01-27 16:53:10,499 : INFO : PROGRESS: at sentence #90000, processed 1497282 words, keeping 25699 word types\n",
      "2018-01-27 16:53:10,548 : INFO : PROGRESS: at sentence #100000, processed 1663384 words, keeping 26999 word types\n",
      "2018-01-27 16:53:10,604 : INFO : PROGRESS: at sentence #110000, processed 1827603 words, keeping 28240 word types\n",
      "2018-01-27 16:53:10,657 : INFO : PROGRESS: at sentence #120000, processed 1994715 words, keeping 29279 word types\n",
      "2018-01-27 16:53:10,721 : INFO : PROGRESS: at sentence #130000, processed 2158219 words, keeping 30349 word types\n",
      "2018-01-27 16:53:10,785 : INFO : PROGRESS: at sentence #140000, processed 2324503 words, keeping 31317 word types\n",
      "2018-01-27 16:53:10,844 : INFO : PROGRESS: at sentence #150000, processed 2490142 words, keeping 32179 word types\n",
      "2018-01-27 16:53:10,895 : INFO : PROGRESS: at sentence #160000, processed 2657425 words, keeping 33173 word types\n",
      "2018-01-27 16:53:10,942 : INFO : PROGRESS: at sentence #170000, processed 2822402 words, keeping 34127 word types\n",
      "2018-01-27 16:53:10,994 : INFO : PROGRESS: at sentence #180000, processed 2988994 words, keeping 35025 word types\n",
      "2018-01-27 16:53:11,043 : INFO : PROGRESS: at sentence #190000, processed 3155085 words, keeping 35720 word types\n",
      "2018-01-27 16:53:11,095 : INFO : PROGRESS: at sentence #200000, processed 3321156 words, keeping 36514 word types\n",
      "2018-01-27 16:53:11,140 : INFO : PROGRESS: at sentence #210000, processed 3486079 words, keeping 37290 word types\n",
      "2018-01-27 16:53:11,190 : INFO : PROGRESS: at sentence #220000, processed 3653440 words, keeping 38121 word types\n",
      "2018-01-27 16:53:11,228 : INFO : PROGRESS: at sentence #230000, processed 3818864 words, keeping 38853 word types\n",
      "2018-01-27 16:53:11,286 : INFO : PROGRESS: at sentence #240000, processed 3985978 words, keeping 39610 word types\n",
      "2018-01-27 16:53:11,346 : INFO : PROGRESS: at sentence #250000, processed 4151614 words, keeping 40416 word types\n",
      "2018-01-27 16:53:11,401 : INFO : PROGRESS: at sentence #260000, processed 4316235 words, keeping 41110 word types\n",
      "2018-01-27 16:53:11,455 : INFO : PROGRESS: at sentence #270000, processed 4479798 words, keeping 41789 word types\n",
      "2018-01-27 16:53:11,508 : INFO : PROGRESS: at sentence #280000, processed 4648025 words, keeping 42516 word types\n",
      "2018-01-27 16:53:11,554 : INFO : PROGRESS: at sentence #290000, processed 4814506 words, keeping 43131 word types\n",
      "2018-01-27 16:53:11,595 : INFO : PROGRESS: at sentence #300000, processed 4976106 words, keeping 43711 word types\n",
      "2018-01-27 16:53:11,661 : INFO : PROGRESS: at sentence #310000, processed 5143927 words, keeping 44395 word types\n",
      "2018-01-27 16:53:11,730 : INFO : PROGRESS: at sentence #320000, processed 5310846 words, keeping 44985 word types\n",
      "2018-01-27 16:53:11,795 : INFO : PROGRESS: at sentence #330000, processed 5474969 words, keeping 45598 word types\n",
      "2018-01-27 16:53:11,855 : INFO : PROGRESS: at sentence #340000, processed 5641955 words, keeping 46200 word types\n",
      "2018-01-27 16:53:11,910 : INFO : PROGRESS: at sentence #350000, processed 5808506 words, keeping 46846 word types\n",
      "2018-01-27 16:53:11,957 : INFO : PROGRESS: at sentence #360000, processed 5971934 words, keeping 47441 word types\n",
      "2018-01-27 16:53:12,007 : INFO : PROGRESS: at sentence #370000, processed 6136171 words, keeping 48012 word types\n",
      "2018-01-27 16:53:12,052 : INFO : PROGRESS: at sentence #380000, processed 6300519 words, keeping 48712 word types\n",
      "2018-01-27 16:53:12,089 : INFO : PROGRESS: at sentence #390000, processed 6468919 words, keeping 49297 word types\n",
      "2018-01-27 16:53:12,131 : INFO : PROGRESS: at sentence #400000, processed 6637032 words, keeping 49905 word types\n",
      "2018-01-27 16:53:12,178 : INFO : PROGRESS: at sentence #410000, processed 6807466 words, keeping 50445 word types\n",
      "2018-01-27 16:53:12,217 : INFO : PROGRESS: at sentence #420000, processed 6974810 words, keeping 50970 word types\n",
      "2018-01-27 16:53:12,277 : INFO : PROGRESS: at sentence #430000, processed 7141392 words, keeping 51572 word types\n",
      "2018-01-27 16:53:12,328 : INFO : PROGRESS: at sentence #440000, processed 7307073 words, keeping 52077 word types\n",
      "2018-01-27 16:53:12,346 : INFO : collected 52339 word types from a corpus of 7381114 raw words and 444454 sentences\n",
      "2018-01-27 16:53:12,347 : INFO : Loading a fresh vocabulary\n",
      "2018-01-27 16:53:12,442 : INFO : min_count=30 retains 6793 unique words (12% of original 52339, drops 45546)\n",
      "2018-01-27 16:53:12,444 : INFO : min_count=30 leaves 7199327 word corpus (97% of original 7381114, drops 181787)\n",
      "2018-01-27 16:53:12,462 : INFO : deleting the raw counts dictionary of 52339 items\n",
      "2018-01-27 16:53:12,465 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2018-01-27 16:53:12,466 : INFO : downsampling leaves estimated 4759464 word corpus (66.1% of prior 7199327)\n",
      "2018-01-27 16:53:12,467 : INFO : estimated required memory for 6793 words and 200 dimensions: 14265300 bytes\n",
      "2018-01-27 16:53:12,488 : INFO : resetting layer weights\n",
      "2018-01-27 16:53:12,602 : INFO : training model with 2 workers on 6793 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-01-27 16:53:13,605 : INFO : PROGRESS: at 2.25% examples, 533782 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:14,606 : INFO : PROGRESS: at 4.11% examples, 489117 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:15,615 : INFO : PROGRESS: at 6.09% examples, 481422 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:16,625 : INFO : PROGRESS: at 8.28% examples, 489932 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:17,631 : INFO : PROGRESS: at 10.26% examples, 485495 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:18,632 : INFO : PROGRESS: at 12.89% examples, 508125 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:19,642 : INFO : PROGRESS: at 15.55% examples, 525051 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:20,648 : INFO : PROGRESS: at 17.61% examples, 520243 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:21,661 : INFO : PROGRESS: at 19.68% examples, 516879 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:22,683 : INFO : PROGRESS: at 21.60% examples, 509853 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:23,701 : INFO : PROGRESS: at 23.97% examples, 514224 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:24,716 : INFO : PROGRESS: at 25.96% examples, 509886 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:25,753 : INFO : PROGRESS: at 28.25% examples, 511297 words/s, in_qsize 3, out_qsize 0\n",
      "2018-01-27 16:53:26,763 : INFO : PROGRESS: at 30.42% examples, 511189 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:27,777 : INFO : PROGRESS: at 32.32% examples, 506672 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:28,779 : INFO : PROGRESS: at 34.84% examples, 512371 words/s, in_qsize 3, out_qsize 0\n",
      "2018-01-27 16:53:29,781 : INFO : PROGRESS: at 37.08% examples, 513245 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:30,808 : INFO : PROGRESS: at 39.32% examples, 514015 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:31,808 : INFO : PROGRESS: at 41.22% examples, 510720 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:32,816 : INFO : PROGRESS: at 43.42% examples, 511386 words/s, in_qsize 3, out_qsize 0\n",
      "2018-01-27 16:53:33,832 : INFO : PROGRESS: at 45.57% examples, 510899 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:34,833 : INFO : PROGRESS: at 47.63% examples, 509904 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:35,839 : INFO : PROGRESS: at 49.83% examples, 510288 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:36,851 : INFO : PROGRESS: at 51.78% examples, 508126 words/s, in_qsize 4, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-27 16:53:37,858 : INFO : PROGRESS: at 53.74% examples, 506178 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:38,861 : INFO : PROGRESS: at 56.20% examples, 509205 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:39,871 : INFO : PROGRESS: at 58.73% examples, 512560 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:40,876 : INFO : PROGRESS: at 60.78% examples, 511652 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:41,880 : INFO : PROGRESS: at 62.99% examples, 512149 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:42,884 : INFO : PROGRESS: at 65.17% examples, 512188 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:43,897 : INFO : PROGRESS: at 67.58% examples, 513916 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:44,900 : INFO : PROGRESS: at 69.80% examples, 514312 words/s, in_qsize 3, out_qsize 0\n",
      "2018-01-27 16:53:45,903 : INFO : PROGRESS: at 71.94% examples, 514079 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:46,915 : INFO : PROGRESS: at 73.65% examples, 510741 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:47,917 : INFO : PROGRESS: at 75.55% examples, 509048 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:48,922 : INFO : PROGRESS: at 77.75% examples, 509330 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:49,935 : INFO : PROGRESS: at 79.89% examples, 509333 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:50,936 : INFO : PROGRESS: at 81.65% examples, 506957 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:51,937 : INFO : PROGRESS: at 83.56% examples, 505700 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:52,948 : INFO : PROGRESS: at 86.23% examples, 508677 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:53,961 : INFO : PROGRESS: at 88.93% examples, 511795 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:54,974 : INFO : PROGRESS: at 91.67% examples, 514898 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:55,979 : INFO : PROGRESS: at 94.38% examples, 517798 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:56,988 : INFO : PROGRESS: at 97.10% examples, 520547 words/s, in_qsize 4, out_qsize 0\n",
      "2018-01-27 16:53:57,992 : INFO : PROGRESS: at 99.70% examples, 522809 words/s, in_qsize 3, out_qsize 0\n",
      "2018-01-27 16:53:58,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-27 16:53:58,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-27 16:53:58,141 : INFO : training on 36905570 raw words (23800579 effective words) took 45.5s, 522659 effective words/s\n",
      "2018-01-27 16:53:58,144 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-01-27 16:53:58,237 : INFO : saving Word2Vec object under 200features_30minwords_10context, separately None\n",
      "2018-01-27 16:53:58,241 : INFO : not storing attribute syn0norm\n",
      "2018-01-27 16:53:58,243 : INFO : not storing attribute cum_table\n",
      "2018-01-27 16:53:58,364 : INFO : saved 200features_30minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it to have clean messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 30   # Minimum word count                        \n",
    "num_workers = 2       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# train word2vec model based on my 50K review sample\n",
    "\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentence_list, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# in case we need to port model without re-training\n",
    "model_name = \"200features_30minwords_10context\"\n",
    "model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-27 16:53:58,378 : INFO : loading Word2Vec object from 200features_30minwords_10context\n",
      "2018-01-27 16:53:58,423 : INFO : loading wv recursively from 200features_30minwords_10context.wv.* with mmap=None\n",
      "2018-01-27 16:53:58,424 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-01-27 16:53:58,426 : INFO : setting ignored attribute cum_table to None\n",
      "2018-01-27 16:53:58,427 : INFO : loaded 200features_30minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# should we need to load the model\n",
    "model = word2vec.Word2Vec.load(\"200features_30minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-27 16:53:58,475 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'chinese', 0.7550970315933228),\n",
       " (u'malaysian', 0.7533572912216187),\n",
       " (u'szechuan', 0.7189332842826843),\n",
       " (u'northern', 0.7153961658477783),\n",
       " (u'hong', 0.697137713432312),\n",
       " (u'mein', 0.6929337978363037),\n",
       " (u'korean', 0.6914548277854919),\n",
       " (u'chow', 0.6905477046966553),\n",
       " (u'hakka', 0.6875939965248108),\n",
       " (u'kong', 0.6864943504333496)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get terms most similar to cantonese\n",
    "model.wv.most_similar(\"cantonese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6793, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.wv.syn0 consists of a feature vector for each work\n",
    "type(model.wv.syn0)\n",
    "# with a min word count of 30, a vocab of 6,793 words as created\n",
    "len(model.wv.vocab)\n",
    "# shape of wv.syn0 should be 6793, 200\n",
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'seafood', 0.6129480600357056),\n",
       " (u'udon', 0.5720325708389282),\n",
       " (u'spaghetti', 0.5618943572044373),\n",
       " (u'noodles', 0.5600389242172241),\n",
       " (u'bibimbap', 0.5471193790435791),\n",
       " (u'noodle', 0.5407552123069763),\n",
       " (u'rice', 0.5379495620727539),\n",
       " (u'vermicelli', 0.5336020588874817),\n",
       " (u'broccoli', 0.5316550731658936),\n",
       " (u'mein', 0.5210241079330444)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple word algebra example:\n",
    "model.wv.most_similar(positive=['pasta','chinese'], negative=['italian'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a feature vector composed of the average of word vectors in\n",
    "# a review's paragraph\n",
    "def convert_review_feature_vector(word_list, model, feature_count):\n",
    "    # initialise array of length feature_count (200 )\n",
    "    feature_vector = np.zeros((feature_count,), dtype='float32')\n",
    "    # stores count of words that are features in learned vocab\n",
    "    word_count = 0.\n",
    "    # convert learned vocab to set for faster processing\n",
    "    vocab_set = set(model.wv.index2word)\n",
    "    # iterate over words in word_list, adding feature vectors together\n",
    "    for word in word_list:\n",
    "        if word in vocab_set:\n",
    "            word_count += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    # finally divide feature_vector by number of words ot get arithmetic vector mean\n",
    "    feature_vector = np.divide(feature_vector, word_count)\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_reviews2 = pd_review[\"text\"].apply(lambda x: process_review(x, remove_stop=True))\n",
    "# creates a 2D array of feature vector of size review count x feature count\n",
    "review_vectors =\\\n",
    "np.array(clean_reviews2.apply(lambda x: \n",
    "                              convert_review_feature_vector(x, model, 200)).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for search:  2.83883404732 seconds.\n"
     ]
    }
   ],
   "source": [
    "# execute this code to compare each individual review with the search string\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "search_string = \"cantonese\"\n",
    "\n",
    "search_vect = convert_review_feature_vector(search_string.split(), model, 200)\n",
    "\n",
    "from scipy import spatial\n",
    "# calculate cosine similarity of search string with review vectors\n",
    "distances = []\n",
    "for rv in review_vectors:\n",
    "    distances.append(np.round(spatial.distance.cosine(search_vect, rv),3))\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for search: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3055  5150 21927 ... 49828 49459 14855]\n",
      "(u'Chopstick House', u'nPatYo3wQ7tcvx7nzOU4GQ', 0.344)\n",
      "(u'Ajisen Ramen', u'6SAfQKe2oM5g_EtcYXyAMg', 0.355)\n",
      "(u'Lotus Garden Hakka Indian Style Chinese', u'TBzgzTFSa7pJXiLD7emYaQ', 0.399)\n",
      "(u'Kaiju', u'6EVBc9kdc3Hd8KZkLVPnGA', 0.417)\n",
      "(u'Chinese Dumpling House', u'ag8gM2YKZkjndCvl2ti7kQ', 0.419)\n",
      "(u\"Lee's Thai Spring Roll\", u'uaCYXxCsZSD3KMg8XiOdwg', 0.424)\n",
      "(u'Sushi Garden', u'pPhuVbly0ZjyrhIhFazANA', 0.427)\n",
      "(u'Jim Chai Kee Wonton Noodle', u'X9ftU-exKhTMOjtr3B52rw', 0.428)\n",
      "(u'Sansotei', u'-BbnAc9YEO6pjvJGEtFbVQ', 0.43)\n",
      "(u'Rol San', u'O1TvPrgkK2bUo5O5aSZ7lw', 0.433)\n",
      "(u'Seor Ak San', u'4twpbw7n4DmsLxAm6-sMkg', 0.433)\n",
      "(u'New Sky Restaurant', u'J_btDyZbIv0hZNjrw56zlA', 0.434)\n",
      "(u'Seor Ak San', u'4twpbw7n4DmsLxAm6-sMkg', 0.441)\n",
      "(u'Bi Bim Bap', u'ruR-mrEaNbFJGnM-WCbcgg', 0.446)\n",
      "(u'Ho Su Bistro', u'QTSCFDPcuROE8UCvGS8Fiw', 0.448)\n",
      "(u'Huibin', u'HuWUIXfaXt9hcP5MKG-Qyg', 0.448)\n",
      "(u'Lime Asian Cuisine', u'Lft-0Xy72YbwRkn_n5hfXA', 0.45)\n",
      "(u'Phoenix Restaurant', u'8f9Tl2gq78wqjtrToOTo7A', 0.452)\n",
      "(u\"Yang's BBQ Restaurant\", u'0Yh6U06nGLjAMwCw6l9-DA', 0.452)\n",
      "(u'Green Papaya', u'kM91Woq__EKVzLjo4dOTaw', 0.454)\n"
     ]
    }
   ],
   "source": [
    "print np.argsort(distances)\n",
    "# print top 20 cosine similarity\n",
    "results = [(pd_review[\"name\"][x], pd_review[\"id\"][x], distances[x]) for x in np.argsort(distances)[:20]]\n",
    "for result in results:\n",
    "    print result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a newer technique which first concatenates all reviews for a particular\n",
    "# resto together.  \n",
    "# the review dataframe row count is reduced to the number of restaurants.\n",
    "# the aggregated review becomes our new document\n",
    "\n",
    "# first group by resto id and aggregate reviews by first converting to list\n",
    "# and then joining\n",
    "concat_query = pd_review.groupby('id')['text'].apply(lambda x: \" \".join(list(x)))\n",
    "# extract unique id restaurant tuples from original dataframe\n",
    "uniq_restaurants = pd_review.loc[:,[\"id\",\"name\"]].drop_duplicates()\n",
    "# join aggregated reviews with unique resto data frame\n",
    "joint_reviews = uniq_restaurants.join(concat_query, on=\"id\").reset_index(drop=True)\n",
    "\n",
    "clean_reviews3 = joint_reviews[\"text\"].apply(lambda x: process_review(x, remove_stop=True))\n",
    "# creates a 2D array of feature vector of size review count x feature count\n",
    "review_vectors2 =\\\n",
    "np.array(clean_reviews3.apply(lambda x: \n",
    "                              convert_review_feature_vector(x, model, 200)).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for search:  0.478939056396 seconds.\n"
     ]
    }
   ],
   "source": [
    "# execute this code to compare each individual review with the search string\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "search_string = \"cantonese\"\n",
    "\n",
    "search_vect = convert_review_feature_vector(search_string.split(), model, 200)\n",
    "\n",
    "from scipy import spatial\n",
    "# calculate cosine similarity of search string with review vectors\n",
    "distances = []\n",
    "for rv in review_vectors2:\n",
    "    distances.append(np.round(spatial.distance.cosine(search_vect, rv),3))\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for search: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Huibin', u'HuWUIXfaXt9hcP5MKG-Qyg', 0.448)\n",
      "(u'Golden House', u'zTJg9_VFyXiQQ0PegucaJg', 0.467)\n",
      "(u'Chopstick House', u'nPatYo3wQ7tcvx7nzOU4GQ', 0.497)\n",
      "(u'Pepper Chili', u'Pejzx2YgZvywGXJo-thfnw', 0.5)\n",
      "(u'River Tai Restaurant', u'Iov02yUKZVj-Z3A3u37ExA', 0.521)\n",
      "(u'Thai Bright', u'UGG8EIfEfAIbyGhgLIX7Mw', 0.529)\n",
      "(u'The 5 Spices Restaurant', u'UI4lGUcqc4YyzXJ2Uqe6BQ', 0.54)\n",
      "(u'Red Mango', u'SXAXzOwp0I2wiA1V3iMtbg', 0.551)\n",
      "(u'Sometime Cafe', u'baY3pCVhwAKyWFXagiOCNw', 0.555)\n",
      "(u'China Ocean', u'hzdFL2bdWohzZ2RM4fiYYQ', 0.565)\n",
      "(u'Sala Modern Thai Kitchen & Bar', u'lIEahf71RLPJ_rFBJ5fqzQ', 0.566)\n",
      "(u'D Pavilion Restaurant Lounge', u'0K-XQZRh_56WCky5REiHmQ', 0.566)\n",
      "(u'TAO Northern Chinese Cuisine', u'bwFxrxHrz9I36awGc-yjjw', 0.57)\n",
      "(u'Chada Thai', u'E7zuWvHH3XoVKJE8yEGIyw', 0.571)\n",
      "(u'Rice & Noodle', u'Mv3pO01Alty1pXQwi-Uy5A', 0.574)\n",
      "(u'Szechuan Express', u'i3jZgPgXPtXbZIjv7obagQ', 0.579)\n",
      "(u'Green Tea Restaurant', u'TBOKIAMxv0OHKJbarNvSeg', 0.582)\n",
      "(u'California Thai', u'Ri_K4vaiRNQjlyutXgadog', 0.583)\n",
      "(u'Viva Mexico', u'ET0E6XneFsguRUzpMXKrag', 0.585)\n",
      "(u'Truly India', u'r9RInq9B_cT1raDM4bNE6Q', 0.587)\n"
     ]
    }
   ],
   "source": [
    "# print top 20 cosine similarity\n",
    "results = [(joint_reviews[\"name\"][x], joint_reviews[\"id\"][x], distances[x]) for x in np.argsort(distances)[:20]]\n",
    "for result in results:\n",
    "    print result\n",
    "    \n",
    "# mixed results here.  The more reviews there are for a few place, the more\n",
    "# penalised the restaurant is.  The mean of the review's representation in vector\n",
    "# space depends on the total number of words. \n",
    "# sometimes shorter reviews (or less reviews) come up trumps\n",
    "# on the other hand, we avoid duplice results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle original reviews, review_vectors for use in application\n",
    "import pickle;\n",
    "pickle_out = open (\"pd_review.pkl\", \"wb\")\n",
    "pickle.dump(pd_review, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open (\"review_vector.pkl\", \"wb\")\n",
    "pickle.dump(review_vectors, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to project onto 2D using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_closestwords(model, word, feature_count):\n",
    "    \n",
    "    arr = np.empty((0,feature_count), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    #np.set_printoptions(suppress=True)\n",
    "    result = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = result[:, 0]\n",
    "    y_coords = result[:, 1]\n",
    "    # display scatter plot\n",
    "    #fig = plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
