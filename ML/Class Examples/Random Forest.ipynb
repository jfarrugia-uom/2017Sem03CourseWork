{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ICS5110 Machine Learning\n",
    "# Practical for \"Evaluation, Imbalanced Data Sets, Bagging, Boosting, Random Forests\"\n",
    "# Requires \"house-votes-1984_train.txt\" and \"house-votes-1984_test.txt\" \n",
    "# (https://github.com/j2kun/decision-trees/blob/master/house-votes-1984.txt)\n",
    "# Works on both Python 2 and Python 3 \n",
    "# (gives different accuracy results when run using the two versions due to random functions working differently).\n",
    "\n",
    "# Add code at the end of this script to make it use decision trees in a random forest. \n",
    "# Try to find a way to beat the score of the single decision tree. \n",
    "# You are free to construct the forest in any way you can think of. At the end we will compare the evaluations returned.\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "\n",
    "#Set seed of the random functions to make the program deterministic.\n",
    "random.seed(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leaf node of the decision tree that returns a class.\n",
    "class LeafNode(object):\n",
    "        \n",
    "    def __init__(self, class_):\n",
    "    # Expects class to return for classification.\n",
    "        self.class_ = class_\n",
    "    \n",
    "    def evaluate(self, input_items):\n",
    "    # Expects the input to classify.\n",
    "        return self.class_\n",
    "        \n",
    "    def pretty_print(self, indentation=0):\n",
    "    # Displays the node in human friendly form.\n",
    "        print('{}RETURN {}'.format(' '*indentation, self.class_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Internal node of the decision tree that takes a single item from the input and passes control to a child node.\n",
    "class InternalNode(object):\n",
    "        \n",
    "    def __init__(self, input_index, value_to_child_dict):\n",
    "        # Expects the index of the input vector item to consider and a dictionary of children \n",
    "        # to pass control to given the possible values of the considered input item.\n",
    "        self.input_index = input_index\n",
    "        self.value_to_child_dict = value_to_child_dict\n",
    "\n",
    "    def evaluate(self, input_items):\n",
    "        # Expects the input to classify.\n",
    "        value = input_items[self.input_index]\n",
    "        child = self.value_to_child_dict[value]\n",
    "        return child.evaluate(input_items)\n",
    "        \n",
    "    def pretty_print(self, indentation=0):\n",
    "        # Displays the node in human friendly form.\n",
    "        for (value, child) in self.value_to_child_dict.items():\n",
    "            print('{}IF input_items[{}] == {}:'.format(' '*indentation, self.input_index, value))\n",
    "            child.pretty_print(indentation+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_frequent_item(items):\n",
    "    # Get the most frequent item in a list (also known as the mode).\n",
    "    freqs = collections.Counter(items) #This gives a dictionary mapping distinct items to their frequencies.\n",
    "\n",
    "    max_freq = None\n",
    "    max_item = None\n",
    "    for (item, freq) in freqs.items():\n",
    "        if max_freq is None or freq > max_freq:\n",
    "            max_freq = freq\n",
    "            max_item = item\n",
    "    return max_item\n",
    "\n",
    "def get_frequencies_entropy(items):\n",
    "    # Get the entropy of a list based on the frequency of its items. \n",
    "    # Returned item is a measure of purity in the list such that the minimum value returned (0) \n",
    "    # is when the list only contains one repeated item (pure) whilst the maximum \n",
    "    # is when the list contains an equal number of each possible different item (completely impure).\n",
    "    freqs = collections.Counter(items) #This gives a dictionary mapping distinct items to their frequencies.\n",
    "    total_freq = sum(freqs.values())\n",
    "    \n",
    "    entropy = 0.0\n",
    "    for (item, freq) in freqs.items():\n",
    "        proportion = float(freq)/total_freq\n",
    "        entropy += -proportion*math.log(proportion)/math.log(2) #-p*log_2(p)\n",
    "    return entropy\n",
    "\n",
    "def get_information_gain(all_items, splits):\n",
    "    # Get the gain in frequencies entropy when a list is broken into a number of sublists. \n",
    "    # Expects the full list (all_items) and a list of sublists (splits) where \n",
    "    # all the items in the sublists came from the full list and every item in the full list is in one of the sublists.\n",
    "    entropy_all = get_frequencies_entropy(all_items)\n",
    "    \n",
    "    weighted_sum_entropy_splits = 0.0\n",
    "    for split in splits:\n",
    "        weighted_sum_entropy_splits += float(len(split))/len(all_items) * get_frequencies_entropy(split)\n",
    "    \n",
    "    information_gain = entropy_all - weighted_sum_entropy_splits\n",
    "    return information_gain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split a training set into sublists according to a particular item position in the inputs such that\n",
    "# each sublist has the same item at that position. \n",
    "# Expects a training set (training_data) which is a list of tuples [(inputs, target)], \n",
    "# an index (input_index) in the input vectors of the training set to use when splitting, \n",
    "# and the full list of possible values that the input position should have. \n",
    "\n",
    "# Returns a tuple consisting of the split training set and the same splits \n",
    "# but with only the targets of the training set.\n",
    "def get_splits(training_data, input_index, values):\n",
    "    \n",
    "    full_splits   = { value: [] for value in values }\n",
    "    target_splits = { value: [] for value in values }\n",
    "    for (input_items, target) in training_data:\n",
    "        value = input_items[input_index]\n",
    "        full_splits[value].append( (input_items, target) )\n",
    "        target_splits[value].append(target)\n",
    "    return (full_splits, target_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The ID3 algorithm for creating a decision tree from a training set. \n",
    "# Expects a training set (training_data) which is a list of tuples [(inputs, target)] \n",
    "# and a dictionary (index_to_values) mapping indexes in the input vectors of the training set \n",
    "# to a list of all the possible values that can reside in that position. \n",
    "# Returns a decision tree which is a LeafNode or an InternalNode. \n",
    "\n",
    "# Note that you can make this algorithm consider only a subset of the input items \n",
    "# by leaving out some indexes in index_to_values.\n",
    "def id3(training_data, index_to_values):\n",
    "    \n",
    "    all_targets = []\n",
    "    for (input_items, target) in training_data:\n",
    "        all_targets.append(target)\n",
    "    most_frequent_target = get_most_frequent_item(all_targets)\n",
    "    \n",
    "    if all( all_targets[i] == all_targets[0] for i in range(1, len(all_targets)) ): \n",
    "        # Check if the training set consists of the same target class throughout by \n",
    "        # checking if every item is equal to the first.\n",
    "        return LeafNode(all_targets[0])\n",
    "    elif len(index_to_values) == 0:\n",
    "        return LeafNode(most_frequent_target)\n",
    "    else:\n",
    "        best_gain = None\n",
    "        best_index = None\n",
    "        best_full_splits = None\n",
    "        for (input_index, values) in index_to_values.items():\n",
    "            (full_splits, target_splits) = get_splits(training_data, input_index, values)\n",
    "            gain = get_information_gain(all_targets, target_splits.values())\n",
    "            if best_gain is None or gain > best_gain:\n",
    "                best_gain  = gain\n",
    "                best_index = input_index\n",
    "                best_full_splits = full_splits\n",
    "\n",
    "        #Create a copy of index_to_values in order to remove the item index that is used to create the current node.\n",
    "        new_index_to_values = dict(index_to_values)\n",
    "        new_index_to_values.pop(best_index)\n",
    "        \n",
    "        value_to_child_dict = dict()\n",
    "        for (value, split) in best_full_splits.items():\n",
    "            if len(split) == 0:\n",
    "                value_to_child_dict[value] = LeafNode(most_frequent_target)\n",
    "            else:\n",
    "                value_to_child_dict[value] = id3(split, new_index_to_values)\n",
    "        return InternalNode(best_index, value_to_child_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset which is split into a training and a testing set using an 80%/20% split (348/87).\n",
    "# The dataset consists of the target class ('R' or 'D') and 16 input items \n",
    "# (all of which can be 'y', 'n', or '?'), separated by commas.\n",
    "with open('house-votes-1984_train_linux.txt', 'r') as f:\n",
    "    rows = [ line.split(',') for line in f.read().split('\\n')[:-1] ]\n",
    "    training_data = [ (row[1:], row[0]) for row in rows ]\n",
    "with open('house-votes-1984_test_linux.txt', 'r') as f:\n",
    "    rows = [ line.split(',') for line in f.read().split('\\n')[:-1] ]\n",
    "    testing_data  = [ (row[1:], row[0]) for row in rows ]\n",
    "\n",
    "#All the 16 input items in the dataset have values that are 'y', 'n', or '?'.\n",
    "index_to_values = { i: [ 'y', 'n', '?' ] for i in range(16) } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree accuracy: 88.51%\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "\n",
    "#Create a single decision tree and evaluate its accuracy on the testing set.\n",
    "\n",
    "tree = id3(training_data, index_to_values)\n",
    "\n",
    "correct = 0\n",
    "for (input_items, target) in testing_data:\n",
    "    output = tree.evaluate(input_items)\n",
    "    if output == target:\n",
    "        correct += 1\n",
    "print('Tree accuracy: {:.2%}'.format(float(correct)/len(testing_data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['n',\n",
       "   'y',\n",
       "   'y',\n",
       "   'n',\n",
       "   'n',\n",
       "   'y',\n",
       "   'y',\n",
       "   'y',\n",
       "   'y',\n",
       "   'n',\n",
       "   'y',\n",
       "   'n',\n",
       "   'n',\n",
       "   'y',\n",
       "   'y',\n",
       "   'y'],\n",
       "  'D')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_bag(1)\n",
    "#get_random_subspace(10)\n",
    "#len(training_data)\n",
    "\n",
    "# 348 training data\n",
    "\n",
    "# \n",
    "# \n",
    "#index_to_values.items()[0][1]\n",
    "#get_random_subspace(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Forest accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Create a forest of decision trees and evaluate its accuracy on the testing set.\n",
    "\n",
    "# This will give you a random subset of training set to use for the 'training_data' parameter of the 'id3' function \n",
    "# which will allow you to use BAGGING. \n",
    "# Just pass in the number of training set items to keep from the 16 available items and \n",
    "# it will return the parameter to use in the 'id3' function.\n",
    "def get_bag(sample_size):    \n",
    "    return random.sample(training_data, sample_size)\n",
    "    \n",
    "# This will give you a random subset of indexes to use for the 'index_to_values' parameter of the 'id3' function \n",
    "# which will allow you to use random subspace method. \n",
    "# Just pass in the number of input items to keep from the 348 available items and \n",
    "# it will return the parameter to use in the 'id3' function.\n",
    "def get_random_subspace(sample_size):    \n",
    "    return dict(random.sample(index_to_values.items(), sample_size))\n",
    "\n",
    "\n",
    "#Put each individual decision tree in this list.\n",
    "forest = [] \n",
    "#forest.append(id3(get_bag(20), get_random_subspace(10)))\n",
    "treeCount= 10\n",
    "for x in range(treeCount):\n",
    "    print x\n",
    "#<YOUR CODE HERE>\n",
    "# example: forest.append(id3(get_bag(2), get_random_subspace(2)))\n",
    "#------------------------\n",
    "\n",
    "correct = 0\n",
    "for (input_items, target) in testing_data:\n",
    "    outputs = [ tree.evaluate(input_items) for tree in forest ]\n",
    "    #Ensemble by picking the most frequent class returned by all the decision trees.\n",
    "    output = get_most_frequent_item(outputs)     \n",
    "    if output == target:\n",
    "        correct += 1\n",
    "print('Forest accuracy: {:.2%}'.format(float(correct)/len(testing_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
